# import necessary packages
from .buggy_code_checker import *
from .get_personalized_solution import *
from .evaluate_fixed_code import *
from .generate_parsons_blocks import *
from .personalize_parsons import *
from .token_compare import *
from .get_parsons_code_distractors import *


def get_parsons_help(api_token, language, dict_buggy_code, personalize_level):
    """
    The main API to connect with coach.py when students request help.
    Get personalized Parsons scaffolding artifacts based on the provided buggy code and other parameters.
    Args:
        api_token: The API token for GenerativeAI -- now only support OpenAI
        language: The programming language of the code
        dict_buggy_code: A dictionary containing the current buggy code, problem description, unittest code, and a predefined example
        personalize_level: The level of personalization to apply (e.g., "Solution", "Multiple")
    Returns:
        A tuple containing the final fixed code, the final Parsons block, the solution generation type, and the generation result type.
    """
    problem_description = dict_buggy_code["Problem Description"].replace("\\n", "\n")
    buggy_code = dict_buggy_code["CF (Code)"].replace("\\n", "\n")
    # can use a function to extract them in the future
    default_start_code = ""
    default_test_code = ""
    unittest_code = dict_buggy_code["Unittest_Code"].replace("\\n", "\n")
    # This can be "LLM-example" or a predefined example code
    predefined_example = dict_buggy_code["Example"].replace("\\n", "\n")
    # check if language is java, keep it as it is, otherwise, assign it to "python" as default
    if language.lower() == "java" or language.lower() == "python":
        language = language.lower()
    else:
        language = "python"  # Default to python if not specified

    if personalize_level == "Solution" or personalize_level == "Multiple":
        cleaned_fixed_code, generation_result_type = generate_personalized_fixed_code(api_token, language, problem_description, buggy_code, default_start_code, default_test_code, predefined_example, unittest_code)
        final_fixed_code = cleaned_fixed_code.lstrip()
    else:
        return "Error: invalid personalize_level"
    
    if personalize_level == "Solution":
        # generate Parsons puzzles with personalization only at the solution level with paired distractors
        final_Parsons_block = generate_multi_personalized_Parsons_blocks("Solution", language, problem_description, buggy_code, final_fixed_code, default_start_code, default_test_code, unittest_code)
    elif personalize_level == "Multiple":
        # generate Parsons puzzles with personalization at both solution and multiple levels with paired distractors and potentially settled block lines
        final_Parsons_block = generate_multi_personalized_Parsons_blocks("Multiple", language, problem_description, buggy_code, final_fixed_code, default_start_code, default_test_code, unittest_code)
    else:
        return "Error: invalid personalize_level"
    
    return final_fixed_code, final_Parsons_block, "personalization", generation_result_type

def request_fixed_code_from_openai(api_token, language, problem_description, buggy_code, default_start_code, default_test_code, example_solution, unittest_code, solution_generation, old_fixed_code, attempt_type, situation, failure_reason, unittest_result):
    """
    Request a fixed code from GenerativeAI based on the buggy code and other parameters.
    Inputs:
        api_token (str): The API token for GenerativeAI -- now only support OpenAI
        language (str): The programming language of the code - Python or Java
        problem_description (str): textual question description
        buggy_code (str): The buggy code provided by the student
        default_start_code (str): The default starting code for the question
        default_test_code (str): The default test code for the question
        example_solution (str): An example solution for the question
        unittest_code (str): The unittest code for the question
        solution_generation (int): The current generation attempt number -- starts from 2, ends at 0
        old_fixed_code (str): The previously generated fixed code, if any
        attempt_type (str): The type of attempt, e.g., "new", "repeat" - the LLM request might be different based on the attempt_type
        situation (str): The situation for the request, e.g., "a correct answer"
        failure_reason (str): The reason for the failure, e.g., "not correct"
        unittest_result (bool): The result of the unittest evaluation for the buggy code
    Outputs:
        cleaned_fixed_code (str): The cleaned fixed code generated by the LLM
        generation_result_type (str): The type of generation result, e.g., "AI_personalized", "example_more_personalized", "example_solution"
    """
    cleaned_buggy_code = clean_student_code(buggy_code, default_test_code)
    if solution_generation <= 0:
        return example_solution.lstrip(), "example_solution"
    
    # For solution_generation >= 1, fix the code and run tests
    fixed_code = get_fixed_code(api_token, language, problem_description, buggy_code, unittest_code, example_solution, attempt_type=attempt_type, situation=situation, old_fixed_code=old_fixed_code)
    unittest_result, cleaned_fixed_code = unittest_evaluation(language, fixed_code, default_start_code, default_test_code, unittest_case=unittest_code)

    print("this-round-result:", unittest_result, cleaned_fixed_code)
    if not unittest_result:
        return example_solution.lstrip(), "example_solution"

    if unittest_result == True:
        similarity_personalized = code_similarity_score(cleaned_buggy_code, cleaned_fixed_code, language)
        similarity_most_common = code_similarity_score(cleaned_buggy_code, example_solution, language)
        
        # For other cases, return the more similar one as the personalized result
        if similarity_personalized >= similarity_most_common:
            return cleaned_fixed_code.lstrip(), "AI_personalized"
        else:
            return example_solution.lstrip(), "example_more_personalized"

    else:
        print("not correct, retrying ... current solution_generation=", solution_generation)
        # If the unittest result is not correct, we will retry with the same solution_generation_type, but will provide the incorrect code as part of the system message (attachment)
        solution_generation -= 1
        return request_fixed_code_from_openai(language, problem_description, buggy_code, default_start_code, default_test_code, example_solution, unittest_code, solution_generation, old_fixed_code=cleaned_fixed_code, attempt_type="repeat", situation="a correct answer", failure_reason="not correct", unittest_result = False)
    
def generate_example_solution(api_token, language, problem_description, unittest_code, predefined_example):
    """
    Generate or retrieve an example solution based on the provided parameters.
    Inputs:
        api_token (str): The API token for GenerativeAI -- now only support OpenAI
        language (str): The programming language of the code - Python or Java
        problem_description (str): textual question description
        unittest_code (str): The unittest code for the question
        predefined_example (str): A predefined example solution or the keyword "LLM-example" to generate one using LLM
    Outputs:
        example_solution (str): The example solution, either generated by LLM or the predefined one
    """
    if predefined_example == "LLM-example":
        example_solution = get_example_solution(api_token, language, problem_description, unittest_code)
    else:
        example_solution = predefined_example
    return example_solution


def generate_personalized_fixed_code(api_token, language, problem_description, buggy_code, default_start_code, default_test_code, predefined_example, unittest_code, API_attempt=0):
    """
    Generate a personalized fixed code based on the buggy code and other parameters.
    Inputs:
        api_token (str): The API token for GenerativeAI -- now only support OpenAI
        language (str): The programming language of the code - python or java
        problem_description (str): textual question description
        buggy_code (str): The buggy code provided by the student
        default_start_code (str): The default starting code for the question, current ""
        default_test_code (str): The default test code for the question, current ""
        predefined_example (str): A predefined example solution or the keyword "LLM-example" to generate one using LLM
        unittest_code (str): The unittest code for the question
        API_attempt (int): The current API attempt number -- starts from 0 (reserved in case we want to use different API keys in the future)
    Outputs:
        cleaned_fixed_code (str): The cleaned fixed code generated by the LLM
        generation_result_type (str): The type of generation result, e.g., "AI_personalized", "example_more_personalized", "example_solution", "written_code_correct", "empty_beginning"
    """
    # check if students contributed any code -- If it is True, then check the correctness
    if bool(student_code_checker(language, buggy_code)):
        # check whether the existing code is already correct
        unittest_result, cleaned_buggy_but_correct_code = unittest_evaluation(language, buggy_code, default_start_code, default_test_code, unittest_case=unittest_code)
        if unittest_result != True:
            # If the code is not correct, we will TRY to get an example solution first
            example_solution = generate_example_solution(api_token, language, problem_description, unittest_code, predefined_example)

            try:
                # The first personalized fixed code attempt
                result_personalized = request_fixed_code_from_openai(
                    api_token, language, problem_description, buggy_code,
                    default_start_code, default_test_code, example_solution,
                    unittest_code, 2, "",
                    attempt_type="new", situation="", failure_reason="", unittest_result=""
                )

                return result_personalized

            except Exception as e:
                # When there is an error from the LLM API, we will return the example solution
                return ("error_personalized", example_solution.lstrip(), "example_solution")
        # If the code is correct, we will return the cleaned_buggy_but_correct_code and raise a message
        else:
            return "written_code_correct", cleaned_buggy_but_correct_code.lstrip(), "written_code"
    # If the code is empty, directly return the example solution
    else:
        example_solution = generate_example_solution(api_token, language, problem_description, unittest_code, predefined_example)

        return "empty_beginning", example_solution.lstrip(), "example_solution"


def generate_multi_personalized_Parsons_blocks(personalize_level, language, problem_description, buggy_code, fixed_code, default_start_code, default_test_code, unittest_code):
    """
    Generate personalized Parsons blocks based on the student buggy code and fixed code.
    Inputs:
        personalize_level (str): The level of personalization to apply (e.g., "Solution", "Multiple")
        language (str): The programming language of the code - python or java
        problem_description (str): textual question description
        buggy_code (str): The buggy code provided by the student
        fixed_code (str): The fixed code generated by the LLM
        default_start_code (str): The default starting code for the question, current ""
        default_test_code (str): The default test code for the question, current ""
        unittest_code (str): The unittest code for the question
    Outputs:
        personalized_Parsons_block (str): The generated personalized Parsons block code
    """
    buggy_code_for_blocks = clean_student_code(buggy_code, default_test_code)
    # add paired distractors on their code when there are some meaningful comparison (one line similarity > a threshold)
    code_comparison_pairs, fixed_lines, removed_lines, unchanged_lines, total_similarity = compare_code(buggy_code_for_blocks, fixed_code, default_start_code, language)

    # decide the types of Parsons problems and generate correspoding distractors
    Parsons_type, distractors = personalize_Parsons_block(language, problem_description, code_comparison_pairs, buggy_code, fixed_lines, removed_lines, unchanged_lines, total_similarity)
    unittest_flag = True
    if len(distractors) > 0:
        for distractor in distractors.copy().items():
            distractor_correct_line = distractor[0]
            # Prepare the code with distractors for unittest evaluation - should not pass the tests this time
            code_with_distrator = generate_code_with_distrator(unchanged_lines, fixed_lines, distractor)
            unittest_flag, cleaned_code_with_distractors = code_distractor_unittest_evaluation(language, code_with_distrator, default_start_code, default_test_code, unittest_code)
            # If the code with distractors passes the unittest, we will remove the distractor from the distractors list
            if unittest_flag == True:
                distractors.pop(distractor_correct_line)

    if Parsons_type == "Correct":
        # if the student code is already correct, no need to generate Parsons problems, just return a message
        return "Correct_Code"
    else:
        # If the student code is incorrect, we will generate Parsons blocks
        personalized_Parsons_block = generate_Parsons_block(personalize_level, language, Parsons_type, problem_description, fixed_code, unchanged_lines, fixed_lines, distractors)

    return personalized_Parsons_block
